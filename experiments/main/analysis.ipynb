{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tools.utils.settings import DefaultPath as defpath\n",
    "from tools.utils.utils import get_mongodb_collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_name = 'main_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "small = True\n",
    "mongoclient, collections = get_mongodb_collections(small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "test_dir = defpath.data_path.tests + '/' + test_name\n",
    "results_dir = test_dir + '/results/extracted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "solvers = [('josie', 'set'), ('josie', 'bag'), ('lshforest', 'set'), ('lshforest', 'bag')]\n",
    "q = '10'\n",
    "\n",
    "results = pd.read_csv(f'{results_dir}/final_results_q{q}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "results['difference_overlap'] = results['algorithm_overlap'] - results['sloth_overlap']\n",
    "results['algorithm_overlap_norm'] = results['algorithm_overlap'] / (results['sloth_overlap'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I vari metodi per caso ritornano risultati che in realtà non hanno overlap reale?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JOSIE no perché esatto, LSHForest può sbagliare (raramente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "nullity_threshold = 0   # if the actual overlap between two tables isprint(next(resgroup)) under this threshold, the result is considered bad\n",
    "\n",
    "x = []\n",
    "for am, am_group in results.groupby(by=[\"algorithm\", \"mode\"]):\n",
    "    for query_id, q_group in am_group.groupby(by=[\"query_id\"]):\n",
    "        cnt = ((q_group['sloth_overlap'] == 0)).sum()\n",
    "        num_query_results = q_group.count().values.tolist()[0]\n",
    "        x.append([am[0], am[1], query_id[0], num_query_results, cnt, cnt / num_query_results])\n",
    "\n",
    "x = pd.DataFrame(x, columns=['algorithm', 'mode', 'query_id', 'query_size', 'zero_overlap_cnt', 'zero_overlap_ratio'])\n",
    "\n",
    "null_ratio_pivot = pd.pivot_table(x, values=['zero_overlap_ratio'], index=['algorithm', 'mode'], aggfunc=['mean', 'std', 'min', 'max'])\n",
    "null_ratio_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm vs True Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data = [(am[0], am[1], group[(group['sloth_overlap'] != 0) & (group['difference_overlap'] != 0)]) for am, group in results.groupby(by=['algorithm', 'mode'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey='row', figsize=(15, 5))\n",
    "xmin, xmax, step = -200, 300, 10\n",
    "\n",
    "ax.hist([d[2]['difference_overlap'] for d in data], \n",
    "         bins=np.arange(xmin, xmax, step), alpha=0.8, \n",
    "         label=[f'{a}-{m}' for a, m, _ in data],\n",
    "         align='mid')\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_xticks(np.arange(xmin, xmax, step))\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "ax.grid()\n",
    "ax.set_xlabel('ALGORITHM overlap - SLOTH overlap')\n",
    "ax.set_ylabel('frequency')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "data = [(am[0], am[1], group[(group['sloth_overlap'] != 0) & (group['difference_overlap'] != 0)]) for am, group in results.groupby(by=['algorithm', 'mode'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, sharey='row', figsize=(15, 5))\n",
    "xmin, xmax, step = 0, 9.6, 0.2\n",
    "\n",
    "ax.hist([d[2]['algorithm_overlap_norm'] for d in data], \n",
    "         bins=np.arange(xmin, xmax, step), alpha=0.8, \n",
    "         label=[f'{a}-{m}' for a, m, _ in data],\n",
    "         align='mid')\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_xticks(np.arange(xmin, xmax, step))\n",
    "ax.grid()\n",
    "ax.set_xlabel('ALGORITHM_overlap / (SLOTH_overlap + 1)')\n",
    "ax.set_ylabel('frequency')\n",
    "ax.set_yscale('log')\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Silver Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "silver_standard = defaultdict(set)\n",
    "\n",
    "results_ids = results.convert_dtypes().groupby(by='query_id')[['result_id', 'sloth_overlap']]\n",
    "\n",
    "for query_id, ids_overlaps in results_ids:\n",
    "    for i in ids_overlaps.values:\n",
    "        _id, _overlap = i\n",
    "        silver_standard[query_id].add((_id, _overlap))\n",
    "\n",
    "for query_id in silver_standard.keys():\n",
    "    silver_standard[query_id] = sorted(list(silver_standard[query_id]), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision at p - $P@p$\n",
    "Corresponds to the number of relevant results among the top $p$ retrieved documents. Fails to take into account the positions of the relevant documents among the top $p$.Another shortcoming is that on a query with fewer relevant results than $p$, even a perfect system will have a score less than 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "k_precisions = [1, 3, 5, 10]\n",
    "\n",
    "precision_at_k_results = []\n",
    "\n",
    "for query_id in silver_standard.keys():\n",
    "    qss = [x[1] for x in silver_standard[query_id]]\n",
    "    avg_overlap = round(statistics.mean(qss), 3)\n",
    "    stdev_overlap = round(statistics.stdev(qss))\n",
    "\n",
    "    for (algorithm, mode), data in results.groupby(by=[\"algorithm\", \"mode\"]):\n",
    "        ids = data[data['query_id'] == query_id]['result_id'].values.tolist()\n",
    "        for k_precision in k_precisions:\n",
    "            real_topk = [x[0] for x in silver_standard[query_id][:k_precision]]\n",
    "            precision_at_k = set(real_topk).intersection(ids)\n",
    "            \n",
    "            precision_at_k_results.append([query_id, len(qss), avg_overlap, stdev_overlap, algorithm, mode, k_precision, len(precision_at_k)])\n",
    "\n",
    "columns = [\n",
    "    'query_id',\n",
    "    'silver_std_size',\n",
    "    'silver_std_ov_mean',\n",
    "    'silver_std_ov_stdev',\n",
    "    'algorithm',\n",
    "    'mode',\n",
    "    'k',\n",
    "    'precision_at_k'\n",
    "]\n",
    "\n",
    "precision_at_k_results = pd.DataFrame(precision_at_k_results, columns=columns)\n",
    "precision_at_k = precision_at_k_results.sort_values(by=['silver_std_size', 'query_id'], ascending=False)\n",
    "# precision_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "patk_pivot = pd.pivot_table(precision_at_k, values=['precision_at_k'], index=['algorithm', 'mode'], columns=['k'], aggfunc=['mean', 'std', 'max'])\n",
    "patk_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for row, label in zip(patk_pivot['mean', 'precision_at_k'].values, patk_pivot.index):\n",
    "    plt.plot([1, 3, 5, 10], row, 'o-', label=f'{label[0]}-{label[1]}')\n",
    "plt.xticks([1, 3, 5, 10], [1, 3, 5, 10])\n",
    "plt.xlabel('p')\n",
    "plt.ylabel('mean precision@p')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalized Discontinued Cumulative Gain - $nDCG@p$\n",
    "\n",
    "Search result lists vary in length depending on the query. Comparing a search engine's performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of $ p $ should be normalized across queries. This is done by sorting all relevant documents in the corpus by their relative relevance, producing the maximum possible DCG through position $p$, also called Ideal DCG (IDCG) through that position. For a query, the normalized discounted cumulative gain, or nDCG, is computed as: \n",
    "\n",
    "$ nDCG_{p} = {DCG_{p} \\over IDCG_{p}} $\n",
    "\n",
    "where $ IDCG_{p} $ is the ideal discounted cumulative gain,\n",
    "\n",
    "$ IDCG_{p} = \\sum_{i=1}^{|REL_{p}|} {2^{rel_{i}} - 1 \\over log_{2}(i + 1)}$\n",
    "\n",
    "where $ REL_{p} $ represents the list of relevant documents (ordered by their relevance) in the corpus up to position $p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def ndcg_at_p(true_relevances, scores, p):\n",
    "    p = min(p, len(true_relevances), len(scores))\n",
    "    if p <= 0: # because computing nDCG is meaningful only if there is more than one document \n",
    "        return 0, 1\n",
    "    idcg = sum(rel / log2(i + 1) for i, rel in enumerate(true_relevances[:p], start=1))\n",
    "    dcg = sum(rel / log2(i + 1) for i, rel in enumerate(scores[:p], start=1))\n",
    "    if idcg < dcg:\n",
    "        raise Exception()\n",
    "\n",
    "    return dcg / idcg, p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def get_nDCG_p(silver_standard:defaultdict[int:list[tuple[int,int]]], results:pd.DataFrame, *p):\n",
    "    \"\"\" p values are assumed positive \"\"\"\n",
    "    ndcg_res = []\n",
    "    for query_id in silver_standard:\n",
    "        true_relevances = [x[1] for x in silver_standard[query_id]]\n",
    "        max_silver_standard = true_relevances[0]\n",
    "\n",
    "        for (algorithm, mode), data in results.groupby(by=['algorithm', 'mode']):\n",
    "            r = data[data['query_id'] == query_id][['result_id', 'sloth_overlap']]\n",
    "            result_relevances = [min(max_silver_standard, x[1]) for x in r.values.tolist()]\n",
    "            for _p in p:\n",
    "                ndcg, _actual_p = ndcg_at_p(true_relevances, result_relevances, _p)\n",
    "                if query_id == 29938 and algorithm == 'josie' and mode == 'bag':\n",
    "                    print(ndcg, _p, true_relevances[:_p], result_relevances[:_p])\n",
    "                ndcg_res.append([query_id, len(true_relevances), algorithm, mode, _p, _p - _actual_p, ndcg])\n",
    "    return ndcg_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "res = get_nDCG_p(silver_standard, results, 1, 3, 5, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(res, columns=['query_id', 'silver_standard_size', 'algorithm', 'mode', 'p', 'missing_p', 'ndcg_p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "silver_standard_size_threshold = 5\n",
    "df = df[df['silver_standard_size'] >= silver_standard_size_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ndcg_pivot = df.pivot_table(index=['algorithm', 'mode'], columns=['p'], values=['ndcg_p', 'missing_p'], aggfunc=['mean', 'max']).convert_dtypes()\n",
    "ndcg_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "ndcg_pivot = df.pivot_table(index=['algorithm', 'mode'], columns=['p'], values=['ndcg_p'], aggfunc=['mean', 'max']).convert_dtypes()\n",
    "ndcg_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "for row, label in zip(ndcg_pivot['mean', 'ndcg_p'].values, ndcg_pivot.index):\n",
    "    plt.plot([1, 3, 5, 10], row, 'o-', label=f'{label[0]}-{label[1]}')\n",
    "plt.xticks([1, 3, 5, 10], [1, 3, 5, 10])\n",
    "plt.legend()\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"mean nDCG@p\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analisi tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "\n",
    "x = [ \n",
    "    (\n",
    "        algorithm, mode, nsamples, \n",
    "        pd.read_csv(test_dir + f'/results/base/a{algorithm}_m{mode}_k10_q{nsamples}.csv')['duration'].sum(),  \n",
    "        pd.read_csv(test_dir + f'/results/base/a{algorithm}_m{mode}_k10_q{nsamples}.csv')['duration'].mean()\n",
    "    )\n",
    "    for algorithm, mode, nsamples in itertools.product(['josie', 'lshforest'], ['set', 'bag'], [\"10\"])\n",
    "]\n",
    "\n",
    "pd.DataFrame(x, columns=['algorithm', 'mode', 'num query', 'top-K time total (s)', 'top-K time mean (s)'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nanni-tesi-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
